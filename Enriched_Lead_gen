!pip install requests pandas tqdm

import requests
import pandas as pd
from tqdm import tqdm
import time

# --- CONFIG ---
API_KEY = "AIzaSyBW12XVD4xOBHzHyxjm051U_jZppZs2gIs"  # üîë Replace this with your key
MAX_RESULTS = 150 # Set the maximum number of results you want to retrieve

# --- INPUTS ---
keyword = input("Enter business type (e.g. 'dentist', 'restaurant', 'marketing agency'): ")
location = input("Enter location (e.g. 'Ikeja Lagos', 'Abuja', 'Port Harcourt'): ")
radius = 10000  # meters

# --- PHASE 1: BASIC PLACE SEARCH WITH PAGINATION ---
search_url = f"https://maps.googleapis.com/maps/api/place/textsearch/json?query={keyword}+in+{location}&radius={radius}&key={API_KEY}"
all_places = []
next_page_token = None

while len(all_places) < MAX_RESULTS:
    if next_page_token:
        search_url = f"https://maps.googleapis.com/maps/api/place/textsearch/json?pagetoken={next_page_token}&key={API_KEY}"
    elif all_places: # Only request first page once
      break # Exit loop if no next_page_token and not the first page


    response = requests.get(search_url)
    data = response.json()

    places = data.get("results", [])
    all_places.extend(places)
    print(f"‚úÖ Found {len(places)} results in this page. Total results: {len(all_places)}")

    next_page_token = data.get("next_page_token")

    if not next_page_token or len(places) == 0:
        break
    time.sleep(2) # Add a delay to avoid rate limiting

print(f"‚úÖ Finished collecting initial results. Total results: {len(all_places)}")

# --- PHASE 2: PLACE DETAILS ENRICHMENT ---
details_url = "https://maps.googleapis.com/maps/api/place/details/json"
enriched_data = []

# Limit the number of places to process based on MAX_RESULTS
places_to_process = all_places[:MAX_RESULTS]


for place in tqdm(places_to_process, desc="Enriching leads"):
    place_id = place.get("place_id")
    params = {
        "place_id": place_id,
        "fields": "name,formatted_address,formatted_phone_number,website,rating,opening_hours,geometry",
        "key": API_KEY
    }
    details_resp = requests.get(details_url, params=params).json()
    details = details_resp.get("result", {})

    enriched_data.append({
        "Name": details.get("name"),
        "Address": details.get("formatted_address"),
        "Phone": details.get("formatted_phone_number"),
        "Website": details.get("website"),
        "Rating": details.get("rating"),
        "Open_Now": details.get("opening_hours", {}).get("open_now") if details.get("opening_hours") else None,
        "Maps_URL": f"https://www.google.com/maps/place/?q=place_id:{place_id}",
        "Location": location # Add the location to the DataFrame
    })

df = pd.DataFrame(enriched_data)

# --- EXPORT ---
file_name = f"{keyword}_{location}_enriched_leads.csv".replace(" ", "_")
df.to_csv(file_name, index=False)

print(f"üíæ Exported {len(df)} enriched leads to '{file_name}'")

from google.colab import files
files.download(file_name)

import requests
import time
import pandas as pd
from datetime import datetime

# === HUBSPOT CONFIGURATION ===
HUBSPOT_API_KEY = "pat-eu1-adf2aaa1-6606-4e45-b98f-cf1fb18ba09f"  # üîë Replace this with your HubSpot private app key
BASE_URL = "https://api.hubapi.com"
COMPANY_URL = f"{BASE_URL}/crm/v3/objects/companies"

headers = {
    "Authorization": f"Bearer {HUBSPOT_API_KEY}",
    "Content-Type": "application/json"
}

# === FUNCTION: CHECK IF COMPANY EXISTS ===
def company_exists(domain=None, name=None):
    """Checks HubSpot for existing company by domain or name."""
    search_url = f"{BASE_URL}/crm/v3/objects/companies/search"
    filters = []

    if domain:
        filters.append({"propertyName": "domain", "operator": "EQ", "value": domain})
    if name and not domain:
        filters.append({"propertyName": "name", "operator": "EQ", "value": name})

    if not filters:
        return False

    payload = {"filterGroups": [{"filters": filters}]}
    resp = requests.post(search_url, headers=headers, json=payload)

    if resp.status_code != 200:
        print(f"‚ö†Ô∏è Search error for {name}: {resp.text}")
        return False

    results = resp.json().get("results", [])
    return len(results) > 0


# === FUNCTION: CREATE COMPANY ===
def create_company(row):
    """Creates company in HubSpot if not already existing."""
    if company_exists(domain=row["Website"], name=row["Name"]):
        return {"Name": row["Name"], "Status": "Duplicate", "Timestamp": datetime.now()}

    data = {
        "properties": {
            "name": row["Name"],
            "address": row["Address"],
            "phone": row["Phone"] or "",
            "domain": row["Website"] if row["Website"] else "",
            "city": row["Location"],
        }
    }

    response = requests.post(COMPANY_URL, headers=headers, json=data)

    if response.status_code == 201:
        print(f"‚úÖ Added company: {row['Name']}")
        return {"Name": row["Name"], "Status": "Added", "Timestamp": datetime.now()}
    else:
        print(f"‚ùå Failed for {row['Name']}: {response.text}")
        return {"Name": row["Name"], "Status": "Failed", "Timestamp": datetime.now()}


# === LOOP THROUGH LEADS AND PUSH TO HUBSPOT ===
sync_log = []

for i, row in df.iterrows():
    result = create_company(row)
    sync_log.append(result)
    time.sleep(0.3)  # Prevent API rate limit issues

# === EXPORT SYNC LOG ===
log_df = pd.DataFrame(sync_log)
log_file_name = f"company_sync_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
log_df.to_csv(log_file_name, index=False)

print(f"üìä Sync Log saved as {log_file_name}")

from google.colab import files
files.download(log_file_name)

